# AI Academy: Multi-Model Approach to AI

**Speaker:** Nicholas Renott, Chief AI Engineer at IBM Client Engineering

---

## The Garden Analogy

You can kind of think of an AI model as a vegetable you want to grow in your garden. Before you buy the seeds, you need to research the weather and water requirements for that plant or else it might die before it ever flowers. And as it grows, you need to evaluate and re-optimize the care you're providing so that it can thrive and bloom. And for an entire garden, you need to do that with every vegetable and make sure that none of them interact with each other harmfully, which is important because you need all of those different vegetables in order to survive. You can't live on carrots alone.

## What is a Multi-Model Approach?

If you want your AI garden to grow, you need to ensure that you're using a variety of vegetables, or in this case, multiple models. That's what we call a multi-model approach, where you have a variety of models for your AI use cases. This means you can pick and choose from different models to find the right one for the right use case, which gives you the opportunity to look at how each of those models is designed as you find the right fit.

### Key Questions to Ask

- Who built it?
- What data was it trained on?
- What guardrails are in place for it?
- What risks and regulations do you need to consider and account for?

## Finding the Right Use Case

The other challenge when it comes to finding the right model for the right use case is identifying the best use case to fit your business needs. And that begins with a prompt.

### Understanding Prompts

A prompt is a textual input or instruction that goes into a large language model to set up the basics of the AI. What a good prompt does is clearly articulate your use case and the problem you're solving with AI.

## The Model Selection Process

### Step 1: Write a Specific Prompt

The first step in the process of choosing a model for your use case is writing a very specific prompt that captures:
- Use case
- User problem
- The ask of the technology
- The guardrails for what good looks like

### Step 2: Research Available Models

Next, you'll research the available models, looking at things like:
- Model size
- Performance
- Costs
- Risks
- Deployment methods

### Step 3: Evaluate and Test

You can then use the information you've collected to evaluate those models against your prompt and identify which of them you first want to test. Start with a large model and work with it until you satisfy your original prompt. Then, try to duplicate the result using smaller models. You're essentially passing the same prompt through different models to experiment and see which works best.

### Step 4: Ongoing Evaluation

That enables you to choose the best model for the use case, but it's not the end of the process. You're going to want to continually evaluate and govern that model with ongoing testing so you assess how it's working based on performance and cost benchmarks.

## Continuous Care and Optimization

It's like that idea of the garden. You need to tend to it, not just plant the seeds and hope for the best. And part of that ongoing care is to continually update the data and the prompt where needed to keep it relevant and also test new models as they become available. You don't want to stick to just one model forever and get locked in as situations change both inside and outside your business.

## Key Factors to Consider

All throughout this process, you want to be constantly thinking about the factors that affect your choice of model. In addition to the three elements of performance, accuracy, reliability, and speed, you also want to consider:
- Size
- Deployment method
- Transparency
- Any potential risks

## Implementation Requirements

That implementation is going to require a team that not only crosses disciplines, but also crosses lines of business. Don't think of it as proprietary to any one department, but treat it as a distinctly collaborative project that takes multiple teams to get up and running.

### Performance Benchmarks

Ensure that this team is ready and able to diagnose performance benchmarks, each of which measures something unique and produces a dataset that shows how everything is being calculated. Without this, you can't make informed decisions about future models and use cases.

## Final Thoughts

Even once your little AI model crop is growing happily, you need to keep taking care of it. In this case, that means continuous testing, governance, and optimization, all of which are essential to keep that model up to date and running optimally.

**Remember:** Models evolve, so your strategy and choices need to do the same. You want to keep growing towards the sun instead of withering on the vine.

---

*Check out the AI Academy archives for deep dives into other aspects of AI for business and watch this space for future episodes.*